<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Ying Shen</title>
    <link>https://yingshen-ys.github.io/project/</link>
      <atom:link href="https://yingshen-ys.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Ying Shen</copyright><lastBuildDate>Sat, 27 Apr 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yingshen-ys.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://yingshen-ys.github.io/project/</link>
    </image>
    
    <item>
      <title>DEPENDENCY PARSING WITH DEEP REINFORCEMENT LEARNING</title>
      <link>https://yingshen-ys.github.io/project/dependency/</link>
      <pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://yingshen-ys.github.io/project/dependency/</guid>
      <description>&lt;p&gt;Dependency parsing is one of the most fundamental tasks in the field of Natural Language Processing. Previous work on transition-based dependency parsing mostly rely on greedy decoding at inference stage, and is prone to the error propagation problem, where one early error can lead the parser to diverge further and further away from the ground truth. In this work, we build a reinforcement learning agent using the Advantage Actor Critic (A2C) algorithm to perform non-greedy decoding with transition-based dependency parser by considering the future rewards. This reinforcement learning framework for non-greedy decoding for dependency parsing can be easily built on top of previous transition-based parsers, hence can benefit from previous parsing models. We perform experiments on the English Penn Treebank (PTB) datasets and demonstrate that our approach can achieve 90.63% unlabeled accuracy, which improves around 0.4% accuracy compared to the supervised neural dependency parser.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>WORDS CAN SHIFT</title>
      <link>https://yingshen-ys.github.io/project/shift-word/</link>
      <pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://yingshen-ys.github.io/project/shift-word/</guid>
      <description>&lt;p&gt;Humans convey their intentions through the usage of both verbal and nonverbal behaviors during face-to-face communication. Speaker intentions often vary dynamically depending on different nonverbal contexts, such as vocal patterns and facial expressions. For example, with the same sentence, “The movie is sick!”, the speaker can convey different sentiments when showing different facial expressions or vocal intonations. Although the speaker is using the same adjective “sick” to describe movies, they could be very excited about the movie or find the movie disappointing by showing opposing nonverbal behaviors. As a result, when modeling human language, it is essential to not only consider the literal meaning of the words but also the nonverbal contexts in which these words appear.&lt;/p&gt;
&lt;p&gt;To better model the meaning of words and sentences in different nonverbal contexts, we seek to capture such dynamics in human language by considering the nonverbal signals as a shift of its verbal representations. Since the visual and acoustic behaviors often have a much higher temporal frequency than words, leading to a sequence of accompanying visual and acoustic “subword” units for each uttered word, we also model the structure of nonverbal behaviors during each word span. To this end, we propose the Recurrent Attended Variation Embedding Network (RAVEN) that models the fine-grained structure of nonverbal “subword” sequences and dynamically shifts word representations based on nonverbal cues. Our proposed model achieves competitive performance on two benchmark datasets for multimodal sentiment analysis and emotion recognition. We also visualize the shifted word representations in different nonverbal contexts and summarize common patterns regarding multimodal variations of word representations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dependency Parsing with Deep Reinforcement Learning</title>
      <link>https://yingshen-ys.github.io/project/rl-dependency/</link>
      <pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://yingshen-ys.github.io/project/rl-dependency/</guid>
      <description>&lt;p&gt;Dependency parsing is one of the most fundamental tasks in the field of Natural Language Processing. Previous work on transition-based dependency parsing mostly rely on greedy decoding at inference stage, and is prone to the error propagation problem, where one early error can lead the parser to diverge further and further away from the ground truth. In this work, we build a reinforcement learning agent using the Advantage Actor Critic (A2C) algorithm to perform non-greedy decoding with transition-based dependency parser by considering the future rewards. This reinforcement learning framework for non-greedy decoding for dependency parsing can be easily built on top of previous transition-based parsers, hence can benefit from previous parsing models. We perform experiments on the English Penn Treebank (PTB) datasets and demonstrate that our approach can achieve 90.63% unlabeled accuracy, which improves around 0.4% accuracy compared to the supervised neural dependency parser.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Efficient Low-rank Multimodal Fusion With Modality-Specific Factors</title>
      <link>https://yingshen-ys.github.io/project/low-rank/</link>
      <pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://yingshen-ys.github.io/project/low-rank/</guid>
      <description>&lt;p&gt;Multimodal research is an emerging field of artificial intelligence, and one of the main research problems in this field is multimodal fusion. The fusion of multimodal data is the process of integrating multiple unimodal representations into one compact multimodal representation. Previous research in this field has exploited the expressiveness of tensors for multimodal representation. However, these methods often suffer from exponential increase in dimensions and in computational complexity introduced by transformation of input into tensor. In this paper, we propose the Low-rank Multimodal Fusion method, which performs multimodal fusion using low-rank tensors to improve efficiency. We evaluate our model on three different tasks: multimodal sentiment analysis, speaker trait analysis, and emotion recognition. Our model achieves competitive results on all these tasks while drastically reducing computational complexity. Additional experiments also show that our model can perform robustly for a wide range of low-rank settings, and is indeed much more efficient in both training and inference compared to other methods that utilize tensor represen- tations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SARA: the Socially Aware Robot Assistant</title>
      <link>https://yingshen-ys.github.io/project/sara/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://yingshen-ys.github.io/project/sara/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Chinese Chess AI</title>
      <link>https://yingshen-ys.github.io/project/chess/</link>
      <pubDate>Wed, 01 Oct 2014 00:00:00 +0000</pubDate>
      <guid>https://yingshen-ys.github.io/project/chess/</guid>
      <description>&lt;p&gt;An artificial intelligence game engine for Chinese chess, which is a popular board game in China. The game is designed with two modes: human-computer and human-human mode.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
