<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning | Ying Shen</title>
    <link>https://yingshen-ys.github.io/tag/reinforcement-learning/</link>
      <atom:link href="https://yingshen-ys.github.io/tag/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Reinforcement Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© Ying Shen</copyright><lastBuildDate>Sat, 27 Apr 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yingshen-ys.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Reinforcement Learning</title>
      <link>https://yingshen-ys.github.io/tag/reinforcement-learning/</link>
    </image>
    
    <item>
      <title>DEPENDENCY PARSING WITH DEEP REINFORCEMENT LEARNING</title>
      <link>https://yingshen-ys.github.io/project/dependency/</link>
      <pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://yingshen-ys.github.io/project/dependency/</guid>
      <description>&lt;p&gt;Dependency parsing is one of the most fundamental tasks in the field of Natural Language Processing. Previous work on transition-based dependency parsing mostly rely on greedy decoding at inference stage, and is prone to the error propagation problem, where one early error can lead the parser to diverge further and further away from the ground truth. In this work, we build a reinforcement learning agent using the Advantage Actor Critic (A2C) algorithm to perform non-greedy decoding with transition-based dependency parser by considering the future rewards. This reinforcement learning framework for non-greedy decoding for dependency parsing can be easily built on top of previous transition-based parsers, hence can benefit from previous parsing models. We perform experiments on the English Penn Treebank (PTB) datasets and demonstrate that our approach can achieve 90.63% unlabeled accuracy, which improves around 0.4% accuracy compared to the supervised neural dependency parser.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dependency Parsing with Deep Reinforcement Learning</title>
      <link>https://yingshen-ys.github.io/project/rl-dependency/</link>
      <pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://yingshen-ys.github.io/project/rl-dependency/</guid>
      <description>&lt;p&gt;Dependency parsing is one of the most fundamental tasks in the field of Natural Language Processing. Previous work on transition-based dependency parsing mostly rely on greedy decoding at inference stage, and is prone to the error propagation problem, where one early error can lead the parser to diverge further and further away from the ground truth. In this work, we build a reinforcement learning agent using the Advantage Actor Critic (A2C) algorithm to perform non-greedy decoding with transition-based dependency parser by considering the future rewards. This reinforcement learning framework for non-greedy decoding for dependency parsing can be easily built on top of previous transition-based parsers, hence can benefit from previous parsing models. We perform experiments on the English Penn Treebank (PTB) datasets and demonstrate that our approach can achieve 90.63% unlabeled accuracy, which improves around 0.4% accuracy compared to the supervised neural dependency parser.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
